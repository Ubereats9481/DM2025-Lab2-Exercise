{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: å³æŸé€¸\n",
    "\n",
    "Student ID: 114062553\n",
    "\n",
    "GitHub ID: Ubereats9481\n",
    "\n",
    "Kaggle name: ubereats 9481\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "1. Combine the id, text, and labels into dataframe.\n",
    "2. Do some simple data cleanup including:\n",
    "    * Fix abnormal characters or decoding errors. (\"FranÃƒÂ§ois Ã¢\\x80\\x93 we\\u2019re excited!\" -> \"FranÃ§ois â€“ weâ€™re excited!\")\n",
    "    * Convert multiple space into single space.\n",
    "    * Because the DeBERTa-large model is very powerful, I try to preserve the original text data so that the model can extract useful information on its own.\n",
    "3. Convert text labels to numeric labels.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "Use DeBERTa tokenizer to tokenize text.\n",
    "(As mentioned earlier, the model is powerful, and I don't need to provide it with any additional text information, it can learn very well from the original text.)\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "Use DeBERTa-large to train on this task (with transformer.AutoModelForSequenceClassification), it get best f1_weighted on epoch 2 (0.6884), and the public mean f1 is 0.6984.\n",
    "Training argement:\n",
    "* Batch size: 16*2 (with gradient_accumulation)\n",
    "* Learning rate: 1e-5\n",
    "* Max text length: 512\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "I tried fine tune LLM model (Qwen3-30B-A3B-4bit) with training data and LoRa with different setting (oversample, increase epochs and batch size, lower learning rate), but the result seems slightly worse than DeBERTa-base model, maybe LLM isn't perfect for this task.\n",
    "\n",
    "Public score:\n",
    "* Fine tune Qwen3-30B: 0.6602\n",
    "* DeBERTa-base: 0.6744\n",
    "* DeBERTa-large: 0.6984 (final)\n",
    "\n",
    "(Fine tuning codes are at the bottom) \n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "In Lab 1, I learned many different text preprocessing and feature engineering techniques, but for modern large models like DeBERTa or LLM, they are able to capture the most detailed information from the original text, enabling the identification of things like sarcastic posts and nonsense. If we do too much preprocessing, some details may be lost, especially in sentiment classification tasks.\n",
    "\n",
    "I will remind myself more often in the future to be careful about the techniques I use when dealing with different tasks and models, as they may not be suitable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (47890, 5)\n",
      "Test shape: (16281, 4)\n",
      "Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ftfy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "DATA_DIR = 'data/dm-lab-2-private-competition'\n",
    "POSTS_PATH = os.path.join(DATA_DIR, 'final_posts.json')\n",
    "IDS_PATH = os.path.join(DATA_DIR, 'data_identification.csv')\n",
    "EMOTION_PATH = os.path.join(DATA_DIR, 'emotion.csv')\n",
    "\n",
    "with open(POSTS_PATH, 'r') as f:\n",
    "    posts_data = json.load(f)\n",
    "\n",
    "# Get id and text from json\n",
    "posts_list = [{\n",
    "    'id': entry['root']['_source']['post']['post_id'],\n",
    "    'text': entry['root']['_source']['post']['text']\n",
    "} for entry in posts_data]\n",
    "\n",
    "df_posts = pd.DataFrame(posts_list)\n",
    "df_ids = pd.read_csv(IDS_PATH)\n",
    "df_emotion = pd.read_csv(EMOTION_PATH)\n",
    "\n",
    "df_full = df_ids.merge(df_posts, on='id', how='left')\n",
    "df_full = df_full.merge(df_emotion, on='id', how='left')\n",
    "\n",
    "def clean_text_for_deberta(text):\n",
    "    # Don't lowercase the text, as DeBERTa is case-sensitive (ALL CAPS indicates strong emotion)\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = ftfy.fix_text(str(text))  # Fix unknown characters\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df_full['text'] = df_full['text'].apply(clean_text_for_deberta)\n",
    "\n",
    "df_train = df_full[df_full['split'] == 'train'].copy()\n",
    "df_test = df_full[df_full['split'] == 'test'].copy()\n",
    "\n",
    "# Change emotion labels to numerical labels\n",
    "le = LabelEncoder()\n",
    "df_train['label'] = le.fit_transform(df_train['emotion'])\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "print(f\"Classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854aff6cc0104a8ea3f4731cb6907cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/47890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ubereats81/.pyenv/versions/dm_lab2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a548029c9cf440988cf03b9cc1ef98fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6001df58fde24401b35d7ccb85ece1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 38312\n",
      "Val Size:   9578\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_ds = Dataset.from_pandas(df_train[['text', 'label']])\n",
    "test_ds = Dataset.from_pandas(df_test[['text']])\n",
    "\n",
    "# change label to classlabel (for stratify_by_column) \n",
    "class_label_feature = ClassLabel(num_classes=len(le.classes_), names=le.classes_.tolist())\n",
    "train_ds = train_ds.cast_column(\"label\", class_label_feature)\n",
    "\n",
    "# ##### Tokenizer Loading (macOS Crash Workaround) #####\n",
    "helper_script = f'''\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_NAME}\")\n",
    "# Save the tokenizer to a file\n",
    "with open(\"/tmp/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "'''\n",
    "with open(\"/tmp/load_tokenizer.py\", \"w\") as f:\n",
    "    f.write(helper_script)\n",
    "subprocess.run([sys.executable, \"/tmp/load_tokenizer.py\"], check=True)\n",
    "\n",
    "# Read the tokenizer file\n",
    "with open(\"/tmp/tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "# #####################################################\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# make sure the distribution of labels is the same in train and val sets\n",
    "tokenized_train_split = tokenized_train.train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed=42, \n",
    "    stratify_by_column=\"label\"\n",
    ")\n",
    "train_dataset = tokenized_train_split['train']\n",
    "eval_dataset = tokenized_train_split['test']\n",
    "\n",
    "print(f\"Train Size: {len(train_dataset)}\")\n",
    "print(f\"Val Size:   {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# macOS Crash Workaround\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Device: MPS (macOS GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using Device: CUDA (Nvidia GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using Device: CPU\")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights_tensor=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights_tensor = class_weights_tensor\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.class_weights_tensor is not None:\n",
    "            # Make sure weights are on the correct device\n",
    "            if self.class_weights_tensor.device != logits.device:\n",
    "                self.class_weights_tensor = self.class_weights_tensor.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights_tensor)\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"f1_weighted\": f1_weighted  # Main focus metric (maybe kaggle score use this rather than fi_macro, so I choose f1_weighted as the metric)\n",
    "    }\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=len(le.classes_)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_deberta_large\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # batch size = 16*2=32\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    fp16=False, # For MPS, set to False\n",
    "    bf16=False, # For MPS, set to False\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=50,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=0,  # For MPS, set to 0\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "print(f\"Effective batch size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Expected epochs: {NUM_EPOCHS} (with early stopping)\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Predicting on Test Set...\")\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# 9. Output\n",
    "y_pred_labels = le.inverse_transform(preds) # Convert back to original labels\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'emotion': y_pred_labels\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_deberta_large.csv\", index=False)\n",
    "print(f\"\\nEmotion Distribution:\")\n",
    "print(submission['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune mlx-community/Qwen3-30B-A3B-4bit\n",
    "\n",
    "# (I use Copilot to support this part code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Oversampling\n",
    "import json\n",
    "import random\n",
    "\n",
    "input_file = 'fine_tune_data.jsonl'\n",
    "output_file = 'fine_tune_data_balanced.jsonl'\n",
    "\n",
    "if os.path.exists(input_file):\n",
    "    # Read all samples\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        all_samples = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Group by emotion\n",
    "    emotion_samples = {}\n",
    "    for sample in all_samples:\n",
    "        # Extract emotion from assistant's response\n",
    "        emotion = sample['messages'][-1]['content'].strip().lower()\n",
    "        if emotion not in emotion_samples:\n",
    "            emotion_samples[emotion] = []\n",
    "        emotion_samples[emotion].append(sample)\n",
    "    \n",
    "    # Calculate target count (use mean or median)\n",
    "    counts = {e: len(samples) for e, samples in emotion_samples.items()}\n",
    "    target_count = int(np.percentile(list(counts.values()), 75))  # Target: 75th percentile\n",
    "    \n",
    "    print(f\"Target number: {target_count} (75th percentile)\")\n",
    "    print(f\"\\nLabels information:\")\n",
    "    \n",
    "    # Oversample minority classes\n",
    "    balanced_samples = []\n",
    "    for emotion, samples in emotion_samples.items():\n",
    "        current_count = len(samples)\n",
    "        if current_count < target_count:\n",
    "            # Oversample\n",
    "            oversample_count = target_count - current_count\n",
    "            oversampled = random.choices(samples, k=oversample_count)\n",
    "            balanced_samples.extend(samples + oversampled)\n",
    "            print(f\"{emotion:10s}: {current_count:5d} â†’ {target_count:5d} (+{oversample_count:5d})\")\n",
    "        else:\n",
    "            # Keep original\n",
    "            balanced_samples.extend(samples)\n",
    "            print(f\"{emotion:10s}: {current_count:5d} (unchanged)\")\n",
    "    \n",
    "    # Shuffle\n",
    "    random.seed(42)\n",
    "    random.shuffle(balanced_samples)\n",
    "    \n",
    "    # Save\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sample in balanced_samples:\n",
    "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"Original data: {len(all_samples)}\")\n",
    "    print(f\"Balenced data: {len(balanced_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing MLX training data with class balancing...\n",
      "âœ… Using balanced data: fine_tune_data_balanced.jsonl\n",
      "\n",
      "ðŸ“ Data prepared in mlx_data/\n",
      "   Train: 65565 samples\n",
      "   Valid: 7286 samples\n",
      "   âœ… Using oversampled data for minority classes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "print(\"Preparing MLX training data with class balancing...\")\n",
    "\n",
    "balanced_file = 'fine_tune_data_balanced.jsonl'\n",
    "original_file = 'fine_tune_data.jsonl'\n",
    "\n",
    "if os.path.exists(balanced_file):\n",
    "    input_file = balanced_file\n",
    "    print(f\"âœ… Using balanced data: {balanced_file}\")\n",
    "elif os.path.exists(original_file):\n",
    "    input_file = original_file\n",
    "    print(f\"âš ï¸ Balanced data not found. Using original data: {original_file}\")\n",
    "    print(f\"   Run class_imbalance_analysis.ipynb to generate balanced data.\")\n",
    "else:\n",
    "    print(f\"âŒ Error: No training data found. Please run prepare_fine_tune_data.py first.\")\n",
    "    input_file = None\n",
    "\n",
    "if input_file:\n",
    "    output_dir = 'mlx_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Shuffle to ensure random split\n",
    "    random.seed(42)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # 90% Train, 10% Valid\n",
    "    split_idx = int(len(lines) * 0.9)\n",
    "    train_lines = lines[:split_idx]\n",
    "    valid_lines = lines[split_idx:]\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'train.jsonl'), 'w', encoding='utf-8') as f:\n",
    "        f.writelines(train_lines)\n",
    "        \n",
    "    with open(os.path.join(output_dir, 'valid.jsonl'), 'w', encoding='utf-8') as f:\n",
    "        f.writelines(valid_lines)\n",
    "        \n",
    "    print(f\"\\nðŸ“ Data prepared in {output_dir}/\")\n",
    "    print(f\"   Train: {len(train_lines)} samples\")\n",
    "    print(f\"   Valid: {len(valid_lines)} samples\")\n",
    "    if input_file == balanced_file:\n",
    "        print(f\"   âœ… Using oversampled data for minority classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 6146 iterations (3 epoch(s) with batch size 32)\n",
      "Learning rate: 1e-05\n",
      "Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.\n",
      "Loading pretrained model\n",
      "Fetching 12 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15321.66it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.462% (140.919M/30532.123M)\n",
      "Starting training..., iters: 6146\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:32<00:00,  3.69s/it]\n",
      "Iter 1: Val loss 3.754, Val took 92.270s\n",
      "Iter 10: Train loss 1.890, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.337, Trained Tokens 26661, Peak mem 46.447 GB\n",
      "Iter 20: Train loss 0.978, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 471.515, Trained Tokens 53151, Peak mem 46.447 GB\n",
      "Iter 30: Train loss 0.958, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 486.030, Trained Tokens 79771, Peak mem 46.447 GB\n",
      "Iter 40: Train loss 0.930, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 485.237, Trained Tokens 106312, Peak mem 46.447 GB\n",
      "Iter 50: Train loss 0.913, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 484.013, Trained Tokens 132804, Peak mem 46.447 GB\n",
      "Iter 60: Train loss 0.933, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 489.515, Trained Tokens 159692, Peak mem 46.447 GB\n",
      "Iter 70: Train loss 0.922, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 464.807, Trained Tokens 186458, Peak mem 52.164 GB\n",
      "Iter 80: Train loss 0.920, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 485.953, Trained Tokens 213067, Peak mem 52.164 GB\n",
      "Iter 90: Train loss 0.892, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.169, Trained Tokens 239618, Peak mem 52.164 GB\n",
      "Iter 100: Train loss 0.931, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 459.712, Trained Tokens 266136, Peak mem 52.164 GB\n",
      "Iter 110: Train loss 0.941, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 468.695, Trained Tokens 293180, Peak mem 52.164 GB\n",
      "Iter 120: Train loss 0.896, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.276, Trained Tokens 319917, Peak mem 52.164 GB\n",
      "Iter 130: Train loss 0.904, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 477.728, Trained Tokens 346707, Peak mem 52.164 GB\n",
      "Iter 140: Train loss 0.893, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.273, Trained Tokens 373378, Peak mem 52.164 GB\n",
      "Iter 150: Train loss 0.887, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 498.156, Trained Tokens 399993, Peak mem 52.164 GB\n",
      "Iter 160: Train loss 0.882, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 508.942, Trained Tokens 426474, Peak mem 52.164 GB\n",
      "Iter 170: Train loss 0.887, Learning Rate 1.000e-05, It/sec 0.179, Tokens/sec 476.116, Trained Tokens 453147, Peak mem 52.164 GB\n",
      "Iter 180: Train loss 0.873, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 470.835, Trained Tokens 479613, Peak mem 52.164 GB\n",
      "Iter 190: Train loss 0.916, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 501.437, Trained Tokens 506436, Peak mem 52.164 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:36<00:00,  3.86s/it]\n",
      "Iter 200: Val loss 0.877, Val took 96.527s\n",
      "Iter 200: Train loss 0.854, Learning Rate 1.000e-05, It/sec 0.179, Tokens/sec 471.105, Trained Tokens 532687, Peak mem 52.164 GB\n",
      "Iter 210: Train loss 0.878, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 483.724, Trained Tokens 559154, Peak mem 52.164 GB\n",
      "Iter 220: Train loss 0.899, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 487.653, Trained Tokens 585856, Peak mem 52.164 GB\n",
      "Iter 230: Train loss 0.889, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 475.237, Trained Tokens 612514, Peak mem 52.164 GB\n",
      "Iter 240: Train loss 0.907, Learning Rate 1.000e-05, It/sec 0.170, Tokens/sec 454.353, Trained Tokens 639233, Peak mem 52.164 GB\n",
      "Iter 250: Train loss 0.882, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.025, Trained Tokens 665818, Peak mem 52.164 GB\n",
      "Iter 260: Train loss 0.886, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 487.424, Trained Tokens 692498, Peak mem 52.164 GB\n",
      "Iter 270: Train loss 0.851, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 494.328, Trained Tokens 718925, Peak mem 52.164 GB\n",
      "Iter 280: Train loss 0.884, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 464.323, Trained Tokens 745635, Peak mem 52.164 GB\n",
      "Iter 290: Train loss 0.899, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 488.923, Trained Tokens 772395, Peak mem 52.164 GB\n",
      "Iter 300: Train loss 0.870, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 508.232, Trained Tokens 798966, Peak mem 52.164 GB\n",
      "Iter 310: Train loss 0.873, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 510.577, Trained Tokens 825639, Peak mem 52.164 GB\n",
      "Iter 320: Train loss 0.928, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 500.150, Trained Tokens 852434, Peak mem 52.164 GB\n",
      "Iter 330: Train loss 0.916, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 468.255, Trained Tokens 879426, Peak mem 52.164 GB\n",
      "Iter 340: Train loss 0.927, Learning Rate 1.000e-05, It/sec 0.170, Tokens/sec 460.797, Trained Tokens 906481, Peak mem 52.164 GB\n",
      "Iter 350: Train loss 0.879, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 511.368, Trained Tokens 933116, Peak mem 52.164 GB\n",
      "Iter 360: Train loss 0.898, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 490.673, Trained Tokens 959962, Peak mem 52.164 GB\n",
      "Iter 370: Train loss 0.877, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 510.915, Trained Tokens 986681, Peak mem 52.164 GB\n",
      "Iter 380: Train loss 0.913, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 467.504, Trained Tokens 1013623, Peak mem 52.164 GB\n",
      "Iter 390: Train loss 0.899, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.805, Trained Tokens 1040196, Peak mem 52.164 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.77s/it]\n",
      "Iter 400: Val loss 0.859, Val took 94.325s\n",
      "Iter 400: Train loss 0.885, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 487.133, Trained Tokens 1066954, Peak mem 52.164 GB\n",
      "Iter 410: Train loss 0.876, Learning Rate 1.000e-05, It/sec 0.180, Tokens/sec 482.276, Trained Tokens 1093726, Peak mem 52.164 GB\n",
      "Iter 420: Train loss 0.876, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.176, Trained Tokens 1120394, Peak mem 52.164 GB\n",
      "Iter 430: Train loss 0.885, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 481.282, Trained Tokens 1146825, Peak mem 52.164 GB\n",
      "Iter 440: Train loss 0.882, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 501.008, Trained Tokens 1173641, Peak mem 52.164 GB\n",
      "Iter 450: Train loss 0.885, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 487.096, Trained Tokens 1200338, Peak mem 52.164 GB\n",
      "Iter 460: Train loss 0.871, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 510.164, Trained Tokens 1226923, Peak mem 52.164 GB\n",
      "Iter 470: Train loss 0.888, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 490.674, Trained Tokens 1253776, Peak mem 52.164 GB\n",
      "Iter 480: Train loss 0.963, Learning Rate 1.000e-05, It/sec 0.166, Tokens/sec 455.537, Trained Tokens 1281137, Peak mem 52.164 GB\n",
      "Iter 490: Train loss 0.859, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 496.491, Trained Tokens 1307718, Peak mem 52.164 GB\n",
      "Iter 500: Train loss 0.862, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 474.391, Trained Tokens 1334333, Peak mem 52.164 GB\n",
      "Iter 510: Train loss 0.871, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 472.037, Trained Tokens 1360835, Peak mem 52.164 GB\n",
      "Iter 520: Train loss 0.887, Learning Rate 1.000e-05, It/sec 0.170, Tokens/sec 455.207, Trained Tokens 1387574, Peak mem 52.164 GB\n",
      "Iter 530: Train loss 0.868, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.640, Trained Tokens 1414235, Peak mem 52.164 GB\n",
      "Iter 540: Train loss 0.865, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.274, Trained Tokens 1440885, Peak mem 52.164 GB\n",
      "Iter 550: Train loss 0.865, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.116, Trained Tokens 1467517, Peak mem 52.164 GB\n",
      "Iter 560: Train loss 0.842, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.431, Trained Tokens 1494140, Peak mem 52.164 GB\n",
      "Iter 570: Train loss 0.854, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.439, Trained Tokens 1520709, Peak mem 52.164 GB\n",
      "Iter 580: Train loss 0.882, Learning Rate 1.000e-05, It/sec 0.195, Tokens/sec 523.247, Trained Tokens 1547488, Peak mem 52.164 GB\n",
      "Iter 590: Train loss 0.871, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.912, Trained Tokens 1574286, Peak mem 52.164 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:33<00:00,  3.76s/it]\n",
      "Iter 600: Val loss 0.843, Val took 93.918s\n",
      "Iter 600: Train loss 0.908, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 488.389, Trained Tokens 1601212, Peak mem 52.164 GB\n",
      "Iter 610: Train loss 0.872, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.245, Trained Tokens 1627829, Peak mem 52.164 GB\n",
      "Iter 614: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0000614_adapters.safetensors.\n",
      "Iter 620: Train loss 0.875, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.306, Trained Tokens 1654506, Peak mem 52.164 GB\n",
      "Iter 630: Train loss 0.859, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.061, Trained Tokens 1681213, Peak mem 52.164 GB\n",
      "Iter 640: Train loss 0.850, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.753, Trained Tokens 1707766, Peak mem 52.164 GB\n",
      "Iter 650: Train loss 0.836, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 480.684, Trained Tokens 1734231, Peak mem 52.164 GB\n",
      "Iter 660: Train loss 0.882, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 487.876, Trained Tokens 1761112, Peak mem 52.164 GB\n",
      "Iter 670: Train loss 0.828, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.461, Trained Tokens 1787526, Peak mem 52.164 GB\n",
      "Iter 680: Train loss 0.842, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 483.435, Trained Tokens 1814127, Peak mem 52.164 GB\n",
      "Iter 690: Train loss 0.847, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.586, Trained Tokens 1840633, Peak mem 52.164 GB\n",
      "Iter 700: Train loss 0.822, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 478.747, Trained Tokens 1867008, Peak mem 52.164 GB\n",
      "Iter 710: Train loss 0.852, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.822, Trained Tokens 1893505, Peak mem 52.164 GB\n",
      "Iter 720: Train loss 0.865, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.177, Trained Tokens 1920277, Peak mem 52.164 GB\n",
      "Iter 730: Train loss 0.867, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 480.455, Trained Tokens 1946723, Peak mem 52.164 GB\n",
      "Iter 740: Train loss 0.849, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.776, Trained Tokens 1973404, Peak mem 52.164 GB\n",
      "Iter 750: Train loss 0.823, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 481.618, Trained Tokens 1999919, Peak mem 52.164 GB\n",
      "Iter 760: Train loss 0.875, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 487.952, Trained Tokens 2026771, Peak mem 52.164 GB\n",
      "Iter 770: Train loss 0.828, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 480.537, Trained Tokens 2053220, Peak mem 52.164 GB\n",
      "Iter 780: Train loss 0.855, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.726, Trained Tokens 2079892, Peak mem 52.164 GB\n",
      "Iter 790: Train loss 0.870, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.125, Trained Tokens 2106674, Peak mem 52.164 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:33<00:00,  3.76s/it]\n",
      "Iter 800: Val loss 0.851, Val took 93.917s\n",
      "Iter 800: Train loss 0.853, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.321, Trained Tokens 2133422, Peak mem 52.164 GB\n",
      "Iter 810: Train loss 0.827, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.936, Trained Tokens 2160003, Peak mem 52.164 GB\n",
      "Iter 820: Train loss 0.887, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 489.316, Trained Tokens 2186947, Peak mem 52.164 GB\n",
      "Iter 830: Train loss 0.858, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.078, Trained Tokens 2213545, Peak mem 52.164 GB\n",
      "Iter 840: Train loss 0.839, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 505.675, Trained Tokens 2240055, Peak mem 52.164 GB\n",
      "Iter 850: Train loss 0.852, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.970, Trained Tokens 2266744, Peak mem 52.164 GB\n",
      "Iter 860: Train loss 0.830, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.795, Trained Tokens 2293531, Peak mem 52.164 GB\n",
      "Iter 870: Train loss 0.858, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.202, Trained Tokens 2320176, Peak mem 52.164 GB\n",
      "Iter 880: Train loss 0.862, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 511.777, Trained Tokens 2346998, Peak mem 52.164 GB\n",
      "Iter 890: Train loss 0.839, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.269, Trained Tokens 2373764, Peak mem 52.164 GB\n",
      "Iter 900: Train loss 0.803, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 466.085, Trained Tokens 2400050, Peak mem 52.164 GB\n",
      "Iter 910: Train loss 0.828, Learning Rate 1.000e-05, It/sec 0.196, Tokens/sec 518.970, Trained Tokens 2426578, Peak mem 52.164 GB\n",
      "Iter 920: Train loss 0.842, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 506.355, Trained Tokens 2453133, Peak mem 52.164 GB\n",
      "Iter 930: Train loss 0.851, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.660, Trained Tokens 2479691, Peak mem 52.164 GB\n",
      "Iter 940: Train loss 0.838, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.815, Trained Tokens 2506316, Peak mem 52.164 GB\n",
      "Iter 950: Train loss 0.832, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.243, Trained Tokens 2533100, Peak mem 52.164 GB\n",
      "Iter 960: Train loss 0.855, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 483.313, Trained Tokens 2559704, Peak mem 52.164 GB\n",
      "Iter 970: Train loss 0.867, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.376, Trained Tokens 2586245, Peak mem 52.164 GB\n",
      "Iter 980: Train loss 0.832, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.946, Trained Tokens 2612980, Peak mem 52.164 GB\n",
      "Iter 990: Train loss 0.847, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.588, Trained Tokens 2639604, Peak mem 52.164 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:35<00:00,  3.83s/it]\n",
      "Iter 1000: Val loss 0.832, Val took 95.766s\n",
      "Iter 1000: Train loss 0.825, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.467, Trained Tokens 2666185, Peak mem 52.164 GB\n",
      "Iter 1010: Train loss 0.828, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.259, Trained Tokens 2692864, Peak mem 52.164 GB\n",
      "Iter 1020: Train loss 0.825, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.423, Trained Tokens 2719558, Peak mem 52.164 GB\n",
      "Iter 1030: Train loss 0.806, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 504.506, Trained Tokens 2746017, Peak mem 52.164 GB\n",
      "Iter 1040: Train loss 0.805, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.456, Trained Tokens 2772508, Peak mem 52.164 GB\n",
      "Iter 1050: Train loss 0.841, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.418, Trained Tokens 2799188, Peak mem 52.164 GB\n",
      "Iter 1060: Train loss 0.828, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.882, Trained Tokens 2825749, Peak mem 52.164 GB\n",
      "Iter 1070: Train loss 0.840, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.130, Trained Tokens 2852416, Peak mem 52.164 GB\n",
      "Iter 1080: Train loss 0.838, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.075, Trained Tokens 2879036, Peak mem 52.164 GB\n",
      "Iter 1090: Train loss 0.837, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 488.235, Trained Tokens 2905939, Peak mem 52.164 GB\n",
      "Iter 1100: Train loss 0.878, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 465.863, Trained Tokens 2932857, Peak mem 52.164 GB\n",
      "Iter 1110: Train loss 0.830, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.683, Trained Tokens 2959396, Peak mem 52.164 GB\n",
      "Iter 1120: Train loss 0.838, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.567, Trained Tokens 2985952, Peak mem 52.728 GB\n",
      "Iter 1130: Train loss 0.844, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.476, Trained Tokens 3012694, Peak mem 52.728 GB\n",
      "Iter 1140: Train loss 0.812, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.635, Trained Tokens 3039230, Peak mem 52.728 GB\n",
      "Iter 1150: Train loss 0.858, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.955, Trained Tokens 3065927, Peak mem 52.728 GB\n",
      "Iter 1160: Train loss 0.860, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 475.295, Trained Tokens 3092748, Peak mem 52.728 GB\n",
      "Iter 1170: Train loss 0.817, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.303, Trained Tokens 3119314, Peak mem 52.728 GB\n",
      "Iter 1180: Train loss 0.799, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 500.247, Trained Tokens 3145558, Peak mem 52.728 GB\n",
      "Iter 1190: Train loss 0.834, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.557, Trained Tokens 3172260, Peak mem 52.728 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:35<00:00,  3.83s/it]\n",
      "Iter 1200: Val loss 0.822, Val took 95.745s\n",
      "Iter 1200: Train loss 0.817, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.458, Trained Tokens 3198797, Peak mem 52.728 GB\n",
      "Iter 1210: Train loss 0.821, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.336, Trained Tokens 3225406, Peak mem 52.728 GB\n",
      "Iter 1220: Train loss 0.838, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.808, Trained Tokens 3252164, Peak mem 52.728 GB\n",
      "Iter 1228: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0001228_adapters.safetensors.\n",
      "Iter 1230: Train loss 0.829, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.522, Trained Tokens 3278819, Peak mem 52.728 GB\n",
      "Iter 1240: Train loss 0.815, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.623, Trained Tokens 3305469, Peak mem 52.728 GB\n",
      "Iter 1250: Train loss 0.797, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.320, Trained Tokens 3332022, Peak mem 52.728 GB\n",
      "Iter 1260: Train loss 0.862, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 499.094, Trained Tokens 3358858, Peak mem 52.728 GB\n",
      "Iter 1270: Train loss 0.815, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.725, Trained Tokens 3385429, Peak mem 52.728 GB\n",
      "Iter 1280: Train loss 0.819, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.707, Trained Tokens 3412073, Peak mem 52.728 GB\n",
      "Iter 1290: Train loss 0.850, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.539, Trained Tokens 3438796, Peak mem 52.728 GB\n",
      "Iter 1300: Train loss 0.863, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 477.539, Trained Tokens 3465731, Peak mem 52.728 GB\n",
      "Iter 1310: Train loss 0.822, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 467.244, Trained Tokens 3492092, Peak mem 52.728 GB\n",
      "Iter 1320: Train loss 0.845, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 464.884, Trained Tokens 3518959, Peak mem 52.728 GB\n",
      "Iter 1330: Train loss 0.797, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.856, Trained Tokens 3545467, Peak mem 52.728 GB\n",
      "Iter 1340: Train loss 0.764, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 477.752, Trained Tokens 3571789, Peak mem 52.728 GB\n",
      "Iter 1350: Train loss 0.820, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.630, Trained Tokens 3598360, Peak mem 52.728 GB\n",
      "Iter 1360: Train loss 0.834, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.067, Trained Tokens 3625067, Peak mem 52.728 GB\n",
      "Iter 1370: Train loss 0.824, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.489, Trained Tokens 3651771, Peak mem 52.728 GB\n",
      "Iter 1380: Train loss 0.808, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 479.879, Trained Tokens 3678205, Peak mem 52.728 GB\n",
      "Iter 1390: Train loss 0.832, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 475.463, Trained Tokens 3705041, Peak mem 52.728 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 1400: Val loss 0.822, Val took 94.834s\n",
      "Iter 1400: Train loss 0.796, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 481.508, Trained Tokens 3731552, Peak mem 52.728 GB\n",
      "Iter 1410: Train loss 0.833, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.540, Trained Tokens 3758205, Peak mem 52.728 GB\n",
      "Iter 1420: Train loss 0.802, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 459.728, Trained Tokens 3784789, Peak mem 52.728 GB\n",
      "Iter 1430: Train loss 0.841, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.918, Trained Tokens 3811684, Peak mem 52.728 GB\n",
      "Iter 1440: Train loss 0.839, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 499.110, Trained Tokens 3838506, Peak mem 52.728 GB\n",
      "Iter 1450: Train loss 0.825, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 462.155, Trained Tokens 3865228, Peak mem 52.728 GB\n",
      "Iter 1460: Train loss 0.827, Learning Rate 1.000e-05, It/sec 0.121, Tokens/sec 332.801, Trained Tokens 3892726, Peak mem 159.323 GB\n",
      "Iter 1470: Train loss 0.801, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.829, Trained Tokens 3919308, Peak mem 159.323 GB\n",
      "Iter 1480: Train loss 0.777, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 476.562, Trained Tokens 3945532, Peak mem 159.323 GB\n",
      "Iter 1490: Train loss 0.806, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.131, Trained Tokens 3972071, Peak mem 159.323 GB\n",
      "Iter 1500: Train loss 0.833, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.499, Trained Tokens 3998873, Peak mem 159.323 GB\n",
      "Iter 1510: Train loss 0.827, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.542, Trained Tokens 4025634, Peak mem 159.323 GB\n",
      "Iter 1520: Train loss 0.776, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 500.506, Trained Tokens 4051896, Peak mem 159.323 GB\n",
      "Iter 1530: Train loss 0.781, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 490.233, Trained Tokens 4078244, Peak mem 159.323 GB\n",
      "Iter 1540: Train loss 0.812, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 463.405, Trained Tokens 4104922, Peak mem 159.323 GB\n",
      "Iter 1550: Train loss 0.830, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.061, Trained Tokens 4131516, Peak mem 159.323 GB\n",
      "Iter 1560: Train loss 0.814, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 488.664, Trained Tokens 4158431, Peak mem 159.323 GB\n",
      "Iter 1570: Train loss 0.773, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 500.473, Trained Tokens 4184676, Peak mem 159.323 GB\n",
      "Iter 1580: Train loss 0.810, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 507.047, Trained Tokens 4211270, Peak mem 159.323 GB\n",
      "Iter 1590: Train loss 0.835, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.642, Trained Tokens 4237973, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 1600: Val loss 0.801, Val took 94.842s\n",
      "Iter 1600: Train loss 0.856, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 479.194, Trained Tokens 4265048, Peak mem 159.323 GB\n",
      "Iter 1610: Train loss 0.807, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 450.694, Trained Tokens 4291713, Peak mem 159.323 GB\n",
      "Iter 1620: Train loss 0.797, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.848, Trained Tokens 4318338, Peak mem 159.323 GB\n",
      "Iter 1630: Train loss 0.796, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.095, Trained Tokens 4344933, Peak mem 159.323 GB\n",
      "Iter 1640: Train loss 0.832, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.350, Trained Tokens 4371669, Peak mem 159.323 GB\n",
      "Iter 1650: Train loss 0.810, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.954, Trained Tokens 4398434, Peak mem 159.323 GB\n",
      "Iter 1660: Train loss 0.800, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 475.102, Trained Tokens 4425252, Peak mem 159.323 GB\n",
      "Iter 1670: Train loss 0.835, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.621, Trained Tokens 4451964, Peak mem 159.323 GB\n",
      "Iter 1680: Train loss 0.798, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.521, Trained Tokens 4478630, Peak mem 159.323 GB\n",
      "Iter 1690: Train loss 0.801, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 478.946, Trained Tokens 4505003, Peak mem 159.323 GB\n",
      "Iter 1700: Train loss 0.786, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.538, Trained Tokens 4531814, Peak mem 159.323 GB\n",
      "Iter 1710: Train loss 0.793, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.983, Trained Tokens 4558478, Peak mem 159.323 GB\n",
      "Iter 1720: Train loss 0.832, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 489.689, Trained Tokens 4585456, Peak mem 159.323 GB\n",
      "Iter 1730: Train loss 0.809, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 464.652, Trained Tokens 4612381, Peak mem 159.323 GB\n",
      "Iter 1740: Train loss 0.810, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.882, Trained Tokens 4638935, Peak mem 159.323 GB\n",
      "Iter 1750: Train loss 0.794, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.723, Trained Tokens 4665575, Peak mem 159.323 GB\n",
      "Iter 1760: Train loss 0.818, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.920, Trained Tokens 4692251, Peak mem 159.323 GB\n",
      "Iter 1770: Train loss 0.831, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 477.934, Trained Tokens 4719255, Peak mem 159.323 GB\n",
      "Iter 1780: Train loss 0.819, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.939, Trained Tokens 4746026, Peak mem 159.323 GB\n",
      "Iter 1790: Train loss 0.834, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.801, Trained Tokens 4772869, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 1800: Val loss 0.785, Val took 94.826s\n",
      "Iter 1800: Train loss 0.799, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.560, Trained Tokens 4799587, Peak mem 159.323 GB\n",
      "Iter 1810: Train loss 0.791, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 479.400, Trained Tokens 4826032, Peak mem 159.323 GB\n",
      "Iter 1820: Train loss 0.790, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.712, Trained Tokens 4852753, Peak mem 159.323 GB\n",
      "Iter 1830: Train loss 0.804, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 504.415, Trained Tokens 4879206, Peak mem 159.323 GB\n",
      "Iter 1840: Train loss 0.776, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.712, Trained Tokens 4905865, Peak mem 159.323 GB\n",
      "Iter 1842: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0001842_adapters.safetensors.\n",
      "Iter 1850: Train loss 0.812, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.319, Trained Tokens 4932635, Peak mem 159.323 GB\n",
      "Iter 1860: Train loss 0.825, Learning Rate 1.000e-05, It/sec 0.165, Tokens/sec 446.600, Trained Tokens 4959656, Peak mem 159.323 GB\n",
      "Iter 1870: Train loss 0.770, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 482.571, Trained Tokens 4986220, Peak mem 159.323 GB\n",
      "Iter 1880: Train loss 0.802, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 453.285, Trained Tokens 5013048, Peak mem 159.323 GB\n",
      "Iter 1890: Train loss 0.793, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.189, Trained Tokens 5039591, Peak mem 159.323 GB\n",
      "Iter 1900: Train loss 0.809, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.072, Trained Tokens 5066512, Peak mem 159.323 GB\n",
      "Iter 1910: Train loss 0.752, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.684, Trained Tokens 5093088, Peak mem 159.323 GB\n",
      "Iter 1920: Train loss 0.796, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.730, Trained Tokens 5119828, Peak mem 159.323 GB\n",
      "Iter 1930: Train loss 0.794, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.563, Trained Tokens 5146655, Peak mem 159.323 GB\n",
      "Iter 1940: Train loss 0.771, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 476.322, Trained Tokens 5172969, Peak mem 159.323 GB\n",
      "Iter 1950: Train loss 0.791, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 453.835, Trained Tokens 5199850, Peak mem 159.323 GB\n",
      "Iter 1960: Train loss 0.805, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.894, Trained Tokens 5226422, Peak mem 159.323 GB\n",
      "Iter 1970: Train loss 0.764, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 477.526, Trained Tokens 5252785, Peak mem 159.323 GB\n",
      "Iter 1980: Train loss 0.824, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.600, Trained Tokens 5279624, Peak mem 159.323 GB\n",
      "Iter 1990: Train loss 0.766, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.097, Trained Tokens 5306354, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.80s/it]\n",
      "Iter 2000: Val loss 0.778, Val took 94.920s\n",
      "Iter 2000: Train loss 0.802, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.392, Trained Tokens 5332840, Peak mem 159.323 GB\n",
      "Iter 2010: Train loss 0.762, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.815, Trained Tokens 5359407, Peak mem 159.323 GB\n",
      "Iter 2020: Train loss 0.810, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.238, Trained Tokens 5386194, Peak mem 159.323 GB\n",
      "Iter 2030: Train loss 0.750, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.581, Trained Tokens 5412792, Peak mem 159.323 GB\n",
      "Iter 2040: Train loss 0.767, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.553, Trained Tokens 5439377, Peak mem 159.323 GB\n",
      "Iter 2050: Train loss 0.761, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.763, Trained Tokens 5465976, Peak mem 159.323 GB\n",
      "Iter 2060: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.699, Trained Tokens 5492616, Peak mem 159.323 GB\n",
      "Iter 2070: Train loss 0.729, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.927, Trained Tokens 5519266, Peak mem 159.323 GB\n",
      "Iter 2080: Train loss 0.729, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.376, Trained Tokens 5545830, Peak mem 159.323 GB\n",
      "Iter 2090: Train loss 0.731, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 503.638, Trained Tokens 5572292, Peak mem 159.323 GB\n",
      "Iter 2100: Train loss 0.730, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.761, Trained Tokens 5599039, Peak mem 159.323 GB\n",
      "Iter 2110: Train loss 0.697, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 477.932, Trained Tokens 5625441, Peak mem 159.323 GB\n",
      "Iter 2120: Train loss 0.762, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.996, Trained Tokens 5652330, Peak mem 159.323 GB\n",
      "Iter 2130: Train loss 0.709, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.618, Trained Tokens 5678815, Peak mem 159.323 GB\n",
      "Iter 2140: Train loss 0.763, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.996, Trained Tokens 5705653, Peak mem 159.323 GB\n",
      "Iter 2150: Train loss 0.723, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.394, Trained Tokens 5732239, Peak mem 159.323 GB\n",
      "Iter 2160: Train loss 0.744, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 487.622, Trained Tokens 5759196, Peak mem 159.323 GB\n",
      "Iter 2170: Train loss 0.695, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 489.525, Trained Tokens 5785582, Peak mem 159.323 GB\n",
      "Iter 2180: Train loss 0.738, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.224, Trained Tokens 5812303, Peak mem 159.323 GB\n",
      "Iter 2190: Train loss 0.729, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.019, Trained Tokens 5838935, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.80s/it]\n",
      "Iter 2200: Val loss 0.785, Val took 94.900s\n",
      "Iter 2200: Train loss 0.725, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.881, Trained Tokens 5865738, Peak mem 159.323 GB\n",
      "Iter 2210: Train loss 0.737, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.721, Trained Tokens 5892453, Peak mem 159.323 GB\n",
      "Iter 2220: Train loss 0.739, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.282, Trained Tokens 5919276, Peak mem 159.323 GB\n",
      "Iter 2230: Train loss 0.720, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.444, Trained Tokens 5946230, Peak mem 159.323 GB\n",
      "Iter 2240: Train loss 0.688, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.013, Trained Tokens 5972768, Peak mem 159.323 GB\n",
      "Iter 2250: Train loss 0.703, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.043, Trained Tokens 5999412, Peak mem 159.323 GB\n",
      "Iter 2260: Train loss 0.685, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.036, Trained Tokens 6026240, Peak mem 159.323 GB\n",
      "Iter 2270: Train loss 0.723, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.798, Trained Tokens 6052985, Peak mem 159.323 GB\n",
      "Iter 2280: Train loss 0.718, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.786, Trained Tokens 6079808, Peak mem 159.323 GB\n",
      "Iter 2290: Train loss 0.701, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.177, Trained Tokens 6106589, Peak mem 159.323 GB\n",
      "Iter 2300: Train loss 0.689, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.059, Trained Tokens 6133296, Peak mem 159.323 GB\n",
      "Iter 2310: Train loss 0.696, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 466.963, Trained Tokens 6159696, Peak mem 159.323 GB\n",
      "Iter 2320: Train loss 0.696, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.872, Trained Tokens 6186476, Peak mem 159.323 GB\n",
      "Iter 2330: Train loss 0.702, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 501.580, Trained Tokens 6212848, Peak mem 159.323 GB\n",
      "Iter 2340: Train loss 0.701, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.068, Trained Tokens 6239562, Peak mem 159.323 GB\n",
      "Iter 2350: Train loss 0.705, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.076, Trained Tokens 6266138, Peak mem 159.323 GB\n",
      "Iter 2360: Train loss 0.746, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.951, Trained Tokens 6292963, Peak mem 159.323 GB\n",
      "Iter 2370: Train loss 0.707, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.550, Trained Tokens 6319689, Peak mem 159.323 GB\n",
      "Iter 2380: Train loss 0.664, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.935, Trained Tokens 6346230, Peak mem 159.323 GB\n",
      "Iter 2390: Train loss 0.703, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 492.355, Trained Tokens 6372756, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:33<00:00,  3.72s/it]\n",
      "Iter 2400: Val loss 0.753, Val took 93.068s\n",
      "Iter 2400: Train loss 0.713, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 503.878, Trained Tokens 6399281, Peak mem 159.323 GB\n",
      "Iter 2410: Train loss 0.719, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.653, Trained Tokens 6426089, Peak mem 159.323 GB\n",
      "Iter 2420: Train loss 0.703, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.843, Trained Tokens 6452915, Peak mem 159.323 GB\n",
      "Iter 2430: Train loss 0.699, Learning Rate 1.000e-05, It/sec 0.176, Tokens/sec 471.685, Trained Tokens 6479643, Peak mem 159.323 GB\n",
      "Iter 2440: Train loss 0.720, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.317, Trained Tokens 6506588, Peak mem 159.323 GB\n",
      "Iter 2450: Train loss 0.746, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.129, Trained Tokens 6533523, Peak mem 159.323 GB\n",
      "Iter 2456: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0002456_adapters.safetensors.\n",
      "Iter 2460: Train loss 0.685, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.287, Trained Tokens 6560111, Peak mem 159.323 GB\n",
      "Iter 2470: Train loss 0.692, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.187, Trained Tokens 6586717, Peak mem 159.323 GB\n",
      "Iter 2480: Train loss 0.713, Learning Rate 1.000e-05, It/sec 0.176, Tokens/sec 471.161, Trained Tokens 6613423, Peak mem 159.323 GB\n",
      "Iter 2490: Train loss 0.723, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.213, Trained Tokens 6640214, Peak mem 159.323 GB\n",
      "Iter 2500: Train loss 0.716, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.589, Trained Tokens 6666958, Peak mem 159.323 GB\n",
      "Iter 2510: Train loss 0.702, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.614, Trained Tokens 6693483, Peak mem 159.323 GB\n",
      "Iter 2520: Train loss 0.733, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.338, Trained Tokens 6720312, Peak mem 159.323 GB\n",
      "Iter 2530: Train loss 0.684, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.723, Trained Tokens 6746905, Peak mem 159.323 GB\n",
      "Iter 2540: Train loss 0.701, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.301, Trained Tokens 6773610, Peak mem 159.323 GB\n",
      "Iter 2550: Train loss 0.685, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.130, Trained Tokens 6800136, Peak mem 159.323 GB\n",
      "Iter 2560: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 488.350, Trained Tokens 6826498, Peak mem 159.323 GB\n",
      "Iter 2570: Train loss 0.668, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 479.860, Trained Tokens 6852994, Peak mem 159.323 GB\n",
      "Iter 2580: Train loss 0.720, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.500, Trained Tokens 6879793, Peak mem 159.323 GB\n",
      "Iter 2590: Train loss 0.760, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 449.117, Trained Tokens 6906435, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.80s/it]\n",
      "Iter 2600: Val loss 0.748, Val took 94.900s\n",
      "Iter 2600: Train loss 0.682, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 493.216, Trained Tokens 6933027, Peak mem 159.323 GB\n",
      "Iter 2610: Train loss 0.726, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 495.324, Trained Tokens 6959744, Peak mem 159.323 GB\n",
      "Iter 2620: Train loss 0.683, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.334, Trained Tokens 6986386, Peak mem 159.323 GB\n",
      "Iter 2630: Train loss 0.690, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.195, Trained Tokens 7013160, Peak mem 159.323 GB\n",
      "Iter 2640: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.643, Trained Tokens 7039755, Peak mem 159.323 GB\n",
      "Iter 2650: Train loss 0.691, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.650, Trained Tokens 7066589, Peak mem 159.323 GB\n",
      "Iter 2660: Train loss 0.718, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 493.832, Trained Tokens 7093237, Peak mem 159.323 GB\n",
      "Iter 2670: Train loss 0.734, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 464.286, Trained Tokens 7120138, Peak mem 159.323 GB\n",
      "Iter 2680: Train loss 0.684, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.656, Trained Tokens 7146741, Peak mem 159.323 GB\n",
      "Iter 2690: Train loss 0.653, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 457.969, Trained Tokens 7173258, Peak mem 159.323 GB\n",
      "Iter 2700: Train loss 0.672, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.043, Trained Tokens 7199810, Peak mem 159.323 GB\n",
      "Iter 2710: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 504.189, Trained Tokens 7226320, Peak mem 159.323 GB\n",
      "Iter 2720: Train loss 0.661, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 505.497, Trained Tokens 7252916, Peak mem 159.323 GB\n",
      "Iter 2730: Train loss 0.702, Learning Rate 1.000e-05, It/sec 0.172, Tokens/sec 461.254, Trained Tokens 7279666, Peak mem 159.323 GB\n",
      "Iter 2740: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.424, Trained Tokens 7306518, Peak mem 159.323 GB\n",
      "Iter 2750: Train loss 0.696, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.788, Trained Tokens 7333228, Peak mem 159.323 GB\n",
      "Iter 2760: Train loss 0.716, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.878, Trained Tokens 7359984, Peak mem 159.323 GB\n",
      "Iter 2770: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.567, Trained Tokens 7386621, Peak mem 159.323 GB\n",
      "Iter 2780: Train loss 0.699, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.909, Trained Tokens 7413311, Peak mem 159.323 GB\n",
      "Iter 2790: Train loss 0.702, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.954, Trained Tokens 7440035, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:35<00:00,  3.83s/it]\n",
      "Iter 2800: Val loss 0.732, Val took 95.853s\n",
      "Iter 2800: Train loss 0.700, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.079, Trained Tokens 7466789, Peak mem 159.323 GB\n",
      "Iter 2810: Train loss 0.700, Learning Rate 1.000e-05, It/sec 0.172, Tokens/sec 460.812, Trained Tokens 7493548, Peak mem 159.323 GB\n",
      "Iter 2820: Train loss 0.690, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 495.239, Trained Tokens 7520263, Peak mem 159.323 GB\n",
      "Iter 2830: Train loss 0.683, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 500.161, Trained Tokens 7546542, Peak mem 159.323 GB\n",
      "Iter 2840: Train loss 0.669, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 494.921, Trained Tokens 7573232, Peak mem 159.323 GB\n",
      "Iter 2850: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.413, Trained Tokens 7599923, Peak mem 159.323 GB\n",
      "Iter 2860: Train loss 0.672, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 503.458, Trained Tokens 7626348, Peak mem 159.323 GB\n",
      "Iter 2870: Train loss 0.694, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.361, Trained Tokens 7652834, Peak mem 159.323 GB\n",
      "Iter 2880: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.172, Tokens/sec 462.555, Trained Tokens 7679700, Peak mem 159.323 GB\n",
      "Iter 2890: Train loss 0.698, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.663, Trained Tokens 7706287, Peak mem 159.323 GB\n",
      "Iter 2900: Train loss 0.717, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 491.361, Trained Tokens 7732740, Peak mem 159.323 GB\n",
      "Iter 2910: Train loss 0.699, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.804, Trained Tokens 7759350, Peak mem 159.323 GB\n",
      "Iter 2920: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 477.201, Trained Tokens 7786325, Peak mem 159.323 GB\n",
      "Iter 2930: Train loss 0.654, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 485.011, Trained Tokens 7812449, Peak mem 159.323 GB\n",
      "Iter 2940: Train loss 0.700, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.081, Trained Tokens 7839144, Peak mem 159.323 GB\n",
      "Iter 2950: Train loss 0.660, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 447.896, Trained Tokens 7865674, Peak mem 159.323 GB\n",
      "Iter 2960: Train loss 0.723, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.787, Trained Tokens 7892424, Peak mem 159.323 GB\n",
      "Iter 2970: Train loss 0.673, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 479.149, Trained Tokens 7918841, Peak mem 159.323 GB\n",
      "Iter 2980: Train loss 0.659, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.162, Trained Tokens 7945530, Peak mem 159.323 GB\n",
      "Iter 2990: Train loss 0.692, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 453.909, Trained Tokens 7972448, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.80s/it]\n",
      "Iter 3000: Val loss 0.732, Val took 94.907s\n",
      "Iter 3000: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.702, Trained Tokens 7999042, Peak mem 159.323 GB\n",
      "Iter 3010: Train loss 0.704, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.264, Trained Tokens 8025569, Peak mem 159.323 GB\n",
      "Iter 3020: Train loss 0.665, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 458.928, Trained Tokens 8052117, Peak mem 159.323 GB\n",
      "Iter 3030: Train loss 0.705, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 489.396, Trained Tokens 8078525, Peak mem 159.323 GB\n",
      "Iter 3040: Train loss 0.661, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 500.070, Trained Tokens 8104803, Peak mem 159.323 GB\n",
      "Iter 3050: Train loss 0.680, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 490.464, Trained Tokens 8131210, Peak mem 159.323 GB\n",
      "Iter 3060: Train loss 0.647, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 459.154, Trained Tokens 8157796, Peak mem 159.323 GB\n",
      "Iter 3070: Train loss 0.660, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 491.200, Trained Tokens 8184264, Peak mem 159.323 GB\n",
      "Iter 3070: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0003070_adapters.safetensors.\n",
      "Iter 3080: Train loss 0.670, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.413, Trained Tokens 8210865, Peak mem 159.323 GB\n",
      "Iter 3090: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 491.048, Trained Tokens 8237310, Peak mem 159.323 GB\n",
      "Iter 3100: Train loss 0.721, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 487.144, Trained Tokens 8264193, Peak mem 159.323 GB\n",
      "Iter 3110: Train loss 0.700, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.803, Trained Tokens 8291056, Peak mem 159.323 GB\n",
      "Iter 3120: Train loss 0.690, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.479, Trained Tokens 8317593, Peak mem 159.323 GB\n",
      "Iter 3130: Train loss 0.653, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 492.421, Trained Tokens 8344109, Peak mem 159.323 GB\n",
      "Iter 3140: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.510, Trained Tokens 8370817, Peak mem 159.323 GB\n",
      "Iter 3150: Train loss 0.674, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 491.588, Trained Tokens 8397312, Peak mem 159.323 GB\n",
      "Iter 3160: Train loss 0.682, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.373, Trained Tokens 8424028, Peak mem 159.323 GB\n",
      "Iter 3170: Train loss 0.640, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.465, Trained Tokens 8450691, Peak mem 159.323 GB\n",
      "Iter 3180: Train loss 0.658, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.852, Trained Tokens 8477382, Peak mem 159.323 GB\n",
      "Iter 3190: Train loss 0.687, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.709, Trained Tokens 8504134, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:37<00:00,  3.91s/it]\n",
      "Iter 3200: Val loss 0.717, Val took 97.651s\n",
      "Iter 3200: Train loss 0.666, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.470, Trained Tokens 8531004, Peak mem 159.323 GB\n",
      "Iter 3210: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 488.042, Trained Tokens 8557324, Peak mem 159.323 GB\n",
      "Iter 3220: Train loss 0.644, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.595, Trained Tokens 8583840, Peak mem 159.323 GB\n",
      "Iter 3230: Train loss 0.658, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.402, Trained Tokens 8610349, Peak mem 159.323 GB\n",
      "Iter 3240: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.633, Trained Tokens 8637224, Peak mem 159.323 GB\n",
      "Iter 3250: Train loss 0.653, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 478.773, Trained Tokens 8663676, Peak mem 159.323 GB\n",
      "Iter 3260: Train loss 0.666, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.831, Trained Tokens 8690380, Peak mem 159.323 GB\n",
      "Iter 3270: Train loss 0.628, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.544, Trained Tokens 8717013, Peak mem 159.323 GB\n",
      "Iter 3280: Train loss 0.687, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.520, Trained Tokens 8743677, Peak mem 159.323 GB\n",
      "Iter 3290: Train loss 0.677, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.392, Trained Tokens 8770460, Peak mem 159.323 GB\n",
      "Iter 3300: Train loss 0.695, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 487.192, Trained Tokens 8797331, Peak mem 159.323 GB\n",
      "Iter 3310: Train loss 0.652, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.989, Trained Tokens 8823944, Peak mem 159.323 GB\n",
      "Iter 3320: Train loss 0.708, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 459.282, Trained Tokens 8850543, Peak mem 159.323 GB\n",
      "Iter 3330: Train loss 0.684, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.983, Trained Tokens 8877124, Peak mem 159.323 GB\n",
      "Iter 3340: Train loss 0.700, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.327, Trained Tokens 8903859, Peak mem 159.323 GB\n",
      "Iter 3350: Train loss 0.697, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.113, Trained Tokens 8930512, Peak mem 159.323 GB\n",
      "Iter 3360: Train loss 0.693, Learning Rate 1.000e-05, It/sec 0.119, Tokens/sec 327.863, Trained Tokens 8958034, Peak mem 159.323 GB\n",
      "Iter 3370: Train loss 0.638, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 487.035, Trained Tokens 8984960, Peak mem 159.323 GB\n",
      "Iter 3380: Train loss 0.672, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.357, Trained Tokens 9011539, Peak mem 159.323 GB\n",
      "Iter 3390: Train loss 0.667, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 459.767, Trained Tokens 9038146, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:36<00:00,  3.87s/it]\n",
      "Iter 3400: Val loss 0.738, Val took 96.711s\n",
      "Iter 3400: Train loss 0.667, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 448.313, Trained Tokens 9064735, Peak mem 159.323 GB\n",
      "Iter 3410: Train loss 0.650, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.834, Trained Tokens 9091294, Peak mem 159.323 GB\n",
      "Iter 3420: Train loss 0.689, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.782, Trained Tokens 9117905, Peak mem 159.323 GB\n",
      "Iter 3430: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 458.650, Trained Tokens 9144463, Peak mem 159.323 GB\n",
      "Iter 3440: Train loss 0.688, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 492.445, Trained Tokens 9171009, Peak mem 159.323 GB\n",
      "Iter 3450: Train loss 0.676, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.677, Trained Tokens 9197664, Peak mem 159.323 GB\n",
      "Iter 3460: Train loss 0.698, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.790, Trained Tokens 9224366, Peak mem 159.323 GB\n",
      "Iter 3470: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 499.065, Trained Tokens 9251270, Peak mem 159.323 GB\n",
      "Iter 3480: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 501.454, Trained Tokens 9277655, Peak mem 159.323 GB\n",
      "Iter 3490: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.679, Trained Tokens 9304379, Peak mem 159.323 GB\n",
      "Iter 3500: Train loss 0.696, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 451.632, Trained Tokens 9331142, Peak mem 159.323 GB\n",
      "Iter 3510: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.430, Trained Tokens 9357695, Peak mem 159.323 GB\n",
      "Iter 3520: Train loss 0.665, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.609, Trained Tokens 9384181, Peak mem 159.323 GB\n",
      "Iter 3530: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.598, Trained Tokens 9411016, Peak mem 159.323 GB\n",
      "Iter 3540: Train loss 0.668, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.171, Trained Tokens 9437721, Peak mem 159.323 GB\n",
      "Iter 3550: Train loss 0.647, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 489.327, Trained Tokens 9464719, Peak mem 159.323 GB\n",
      "Iter 3560: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.829, Trained Tokens 9491393, Peak mem 159.323 GB\n",
      "Iter 3570: Train loss 0.666, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.912, Trained Tokens 9518056, Peak mem 159.323 GB\n",
      "Iter 3580: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 477.475, Trained Tokens 9544491, Peak mem 159.323 GB\n",
      "Iter 3590: Train loss 0.662, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.651, Trained Tokens 9571080, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:37<00:00,  3.91s/it]\n",
      "Iter 3600: Val loss 0.703, Val took 97.632s\n",
      "Iter 3600: Train loss 0.677, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.119, Trained Tokens 9597815, Peak mem 159.323 GB\n",
      "Iter 3610: Train loss 0.619, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 504.782, Trained Tokens 9624359, Peak mem 159.323 GB\n",
      "Iter 3620: Train loss 0.655, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.739, Trained Tokens 9650940, Peak mem 159.323 GB\n",
      "Iter 3630: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 467.990, Trained Tokens 9677439, Peak mem 159.323 GB\n",
      "Iter 3640: Train loss 0.690, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.985, Trained Tokens 9704151, Peak mem 159.323 GB\n",
      "Iter 3650: Train loss 0.723, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.861, Trained Tokens 9730746, Peak mem 159.323 GB\n",
      "Iter 3660: Train loss 0.655, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 492.660, Trained Tokens 9757324, Peak mem 159.323 GB\n",
      "Iter 3670: Train loss 0.665, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 499.279, Trained Tokens 9784236, Peak mem 159.323 GB\n",
      "Iter 3680: Train loss 0.660, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 462.018, Trained Tokens 9811008, Peak mem 159.323 GB\n",
      "Iter 3684: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0003684_adapters.safetensors.\n",
      "Iter 3690: Train loss 0.665, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 451.319, Trained Tokens 9837775, Peak mem 159.323 GB\n",
      "Iter 3700: Train loss 0.630, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 508.276, Trained Tokens 9864501, Peak mem 159.323 GB\n",
      "Iter 3710: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.713, Trained Tokens 9891287, Peak mem 159.323 GB\n",
      "Iter 3720: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 499.195, Trained Tokens 9918167, Peak mem 159.323 GB\n",
      "Iter 3730: Train loss 0.676, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.257, Trained Tokens 9945021, Peak mem 159.323 GB\n",
      "Iter 3740: Train loss 0.623, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 487.637, Trained Tokens 9971288, Peak mem 159.323 GB\n",
      "Iter 3750: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.730, Trained Tokens 9998049, Peak mem 159.323 GB\n",
      "Iter 3760: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.319, Trained Tokens 10024585, Peak mem 159.323 GB\n",
      "Iter 3770: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.571, Trained Tokens 10051343, Peak mem 159.323 GB\n",
      "Iter 3780: Train loss 0.671, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.891, Trained Tokens 10078158, Peak mem 159.323 GB\n",
      "Iter 3790: Train loss 0.678, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.956, Trained Tokens 10104864, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:33<00:00,  3.76s/it]\n",
      "Iter 3800: Val loss 0.706, Val took 93.942s\n",
      "Iter 3800: Train loss 0.649, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 488.703, Trained Tokens 10131846, Peak mem 159.323 GB\n",
      "Iter 3810: Train loss 0.627, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.272, Trained Tokens 10158471, Peak mem 159.323 GB\n",
      "Iter 3820: Train loss 0.684, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 469.543, Trained Tokens 10185613, Peak mem 159.323 GB\n",
      "Iter 3830: Train loss 0.669, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 500.281, Trained Tokens 10212509, Peak mem 159.323 GB\n",
      "Iter 3840: Train loss 0.671, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 500.308, Trained Tokens 10239409, Peak mem 159.323 GB\n",
      "Iter 3850: Train loss 0.650, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 468.754, Trained Tokens 10265897, Peak mem 159.323 GB\n",
      "Iter 3860: Train loss 0.645, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.621, Trained Tokens 10292689, Peak mem 159.323 GB\n",
      "Iter 3870: Train loss 0.631, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 480.927, Trained Tokens 10319167, Peak mem 159.323 GB\n",
      "Iter 3880: Train loss 0.636, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.609, Trained Tokens 10345909, Peak mem 159.323 GB\n",
      "Iter 3890: Train loss 0.636, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.973, Trained Tokens 10372619, Peak mem 159.323 GB\n",
      "Iter 3900: Train loss 0.651, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 462.225, Trained Tokens 10399377, Peak mem 159.323 GB\n",
      "Iter 3910: Train loss 0.650, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 478.824, Trained Tokens 10425755, Peak mem 159.323 GB\n",
      "Iter 3920: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.012, Trained Tokens 10452527, Peak mem 159.323 GB\n",
      "Iter 3930: Train loss 0.644, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.962, Trained Tokens 10479027, Peak mem 159.323 GB\n",
      "Iter 3940: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.520, Trained Tokens 10505573, Peak mem 159.323 GB\n",
      "Iter 3950: Train loss 0.649, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.497, Trained Tokens 10532226, Peak mem 159.323 GB\n",
      "Iter 3960: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.195, Tokens/sec 513.911, Trained Tokens 10558540, Peak mem 159.323 GB\n",
      "Iter 3970: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.146, Trained Tokens 10585346, Peak mem 159.323 GB\n",
      "Iter 3980: Train loss 0.681, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.171, Trained Tokens 10612098, Peak mem 159.323 GB\n",
      "Iter 3990: Train loss 0.666, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.130, Trained Tokens 10638766, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 4000: Val loss 0.685, Val took 94.853s\n",
      "Iter 4000: Train loss 0.654, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.872, Trained Tokens 10665447, Peak mem 159.323 GB\n",
      "Iter 4010: Train loss 0.682, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.728, Trained Tokens 10692165, Peak mem 159.323 GB\n",
      "Iter 4020: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 462.251, Trained Tokens 10718853, Peak mem 159.323 GB\n",
      "Iter 4030: Train loss 0.675, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.395, Trained Tokens 10745624, Peak mem 159.323 GB\n",
      "Iter 4040: Train loss 0.618, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.183, Trained Tokens 10772294, Peak mem 159.323 GB\n",
      "Iter 4050: Train loss 0.638, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 475.243, Trained Tokens 10799126, Peak mem 159.323 GB\n",
      "Iter 4060: Train loss 0.612, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.356, Trained Tokens 10825652, Peak mem 159.323 GB\n",
      "Iter 4070: Train loss 0.656, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.503, Trained Tokens 10852171, Peak mem 159.323 GB\n",
      "Iter 4080: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.009, Trained Tokens 10878805, Peak mem 159.323 GB\n",
      "Iter 4090: Train loss 0.648, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 492.267, Trained Tokens 10905275, Peak mem 159.323 GB\n",
      "Iter 4100: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.632, Trained Tokens 10931978, Peak mem 159.323 GB\n",
      "Iter 4110: Train loss 0.604, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.597, Trained Tokens 10958687, Peak mem 159.323 GB\n",
      "Iter 4120: Train loss 0.581, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 505.246, Trained Tokens 10985202, Peak mem 159.323 GB\n",
      "Iter 4130: Train loss 0.557, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 449.308, Trained Tokens 11011804, Peak mem 159.323 GB\n",
      "Iter 4140: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 455.568, Trained Tokens 11038758, Peak mem 159.323 GB\n",
      "Iter 4150: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.455, Trained Tokens 11065423, Peak mem 159.323 GB\n",
      "Iter 4160: Train loss 0.545, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.621, Trained Tokens 11092253, Peak mem 159.323 GB\n",
      "Iter 4170: Train loss 0.591, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 479.963, Trained Tokens 11118691, Peak mem 159.323 GB\n",
      "Iter 4180: Train loss 0.580, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.460, Trained Tokens 11145307, Peak mem 159.323 GB\n",
      "Iter 4190: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.903, Trained Tokens 11172031, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:36<00:00,  3.87s/it]\n",
      "Iter 4200: Val loss 0.687, Val took 96.687s\n",
      "Iter 4200: Train loss 0.556, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.905, Trained Tokens 11198567, Peak mem 159.323 GB\n",
      "Iter 4210: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 483.864, Trained Tokens 11225220, Peak mem 159.323 GB\n",
      "Iter 4220: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.600, Trained Tokens 11251947, Peak mem 159.323 GB\n",
      "Iter 4230: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 490.145, Trained Tokens 11278325, Peak mem 159.323 GB\n",
      "Iter 4240: Train loss 0.543, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 479.043, Trained Tokens 11304753, Peak mem 159.323 GB\n",
      "Iter 4250: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.697, Trained Tokens 11331444, Peak mem 159.323 GB\n",
      "Iter 4260: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.980, Trained Tokens 11358091, Peak mem 159.323 GB\n",
      "Iter 4270: Train loss 0.588, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.065, Trained Tokens 11384760, Peak mem 159.323 GB\n",
      "Iter 4280: Train loss 0.579, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.088, Trained Tokens 11411306, Peak mem 159.323 GB\n",
      "Iter 4290: Train loss 0.588, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 467.984, Trained Tokens 11438388, Peak mem 159.323 GB\n",
      "Iter 4298: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0004298_adapters.safetensors.\n",
      "Iter 4300: Train loss 0.601, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.701, Trained Tokens 11464975, Peak mem 159.323 GB\n",
      "Iter 4310: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.294, Trained Tokens 11491573, Peak mem 159.323 GB\n",
      "Iter 4320: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 512.125, Trained Tokens 11518481, Peak mem 159.323 GB\n",
      "Iter 4330: Train loss 0.590, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.607, Trained Tokens 11545189, Peak mem 159.323 GB\n",
      "Iter 4340: Train loss 0.568, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.038, Trained Tokens 11571752, Peak mem 159.323 GB\n",
      "Iter 4350: Train loss 0.562, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.008, Trained Tokens 11598356, Peak mem 159.323 GB\n",
      "Iter 4360: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 488.485, Trained Tokens 11625278, Peak mem 159.323 GB\n",
      "Iter 4370: Train loss 0.582, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.214, Trained Tokens 11651968, Peak mem 159.323 GB\n",
      "Iter 4380: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.439, Trained Tokens 11678872, Peak mem 159.323 GB\n",
      "Iter 4390: Train loss 0.586, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.420, Trained Tokens 11705631, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:33<00:00,  3.72s/it]\n",
      "Iter 4400: Val loss 0.709, Val took 93.031s\n",
      "Iter 4400: Train loss 0.563, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 501.650, Trained Tokens 11731962, Peak mem 159.323 GB\n",
      "Iter 4410: Train loss 0.554, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.816, Trained Tokens 11758674, Peak mem 159.323 GB\n",
      "Iter 4420: Train loss 0.525, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 491.759, Trained Tokens 11785138, Peak mem 159.323 GB\n",
      "Iter 4430: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 475.657, Trained Tokens 11812008, Peak mem 159.323 GB\n",
      "Iter 4440: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.654, Trained Tokens 11838669, Peak mem 159.323 GB\n",
      "Iter 4450: Train loss 0.550, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.524, Trained Tokens 11865451, Peak mem 159.323 GB\n",
      "Iter 4460: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.145, Trained Tokens 11892193, Peak mem 159.323 GB\n",
      "Iter 4470: Train loss 0.586, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.337, Trained Tokens 11918908, Peak mem 159.323 GB\n",
      "Iter 4480: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.856, Trained Tokens 11945666, Peak mem 159.323 GB\n",
      "Iter 4490: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.966, Trained Tokens 11972351, Peak mem 159.323 GB\n",
      "Iter 4500: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.716, Trained Tokens 11999291, Peak mem 159.323 GB\n",
      "Iter 4510: Train loss 0.553, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 463.209, Trained Tokens 12026112, Peak mem 159.323 GB\n",
      "Iter 4520: Train loss 0.578, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.820, Trained Tokens 12052938, Peak mem 159.323 GB\n",
      "Iter 4530: Train loss 0.578, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 480.455, Trained Tokens 12079420, Peak mem 159.323 GB\n",
      "Iter 4540: Train loss 0.568, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.245, Trained Tokens 12106030, Peak mem 159.323 GB\n",
      "Iter 4550: Train loss 0.545, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 500.072, Trained Tokens 12132290, Peak mem 159.323 GB\n",
      "Iter 4560: Train loss 0.590, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 496.357, Trained Tokens 12159063, Peak mem 159.323 GB\n",
      "Iter 4570: Train loss 0.602, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.841, Trained Tokens 12185784, Peak mem 159.323 GB\n",
      "Iter 4580: Train loss 0.599, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 468.021, Trained Tokens 12212844, Peak mem 159.323 GB\n",
      "Iter 4590: Train loss 0.606, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.230, Trained Tokens 12239630, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:35<00:00,  3.83s/it]\n",
      "Iter 4600: Val loss 0.697, Val took 95.757s\n",
      "Iter 4600: Train loss 0.568, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.204, Trained Tokens 12266187, Peak mem 159.323 GB\n",
      "Iter 4610: Train loss 0.577, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 474.061, Trained Tokens 12292960, Peak mem 159.323 GB\n",
      "Iter 4620: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.371, Trained Tokens 12319596, Peak mem 159.323 GB\n",
      "Iter 4630: Train loss 0.619, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 461.449, Trained Tokens 12346294, Peak mem 159.323 GB\n",
      "Iter 4640: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.232, Trained Tokens 12372960, Peak mem 159.323 GB\n",
      "Iter 4650: Train loss 0.568, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.275, Trained Tokens 12399679, Peak mem 159.323 GB\n",
      "Iter 4660: Train loss 0.581, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.884, Trained Tokens 12426359, Peak mem 159.323 GB\n",
      "Iter 4670: Train loss 0.553, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.255, Trained Tokens 12453104, Peak mem 159.323 GB\n",
      "Iter 4680: Train loss 0.580, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 491.716, Trained Tokens 12479542, Peak mem 159.323 GB\n",
      "Iter 4690: Train loss 0.568, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.896, Trained Tokens 12506113, Peak mem 159.323 GB\n",
      "Iter 4700: Train loss 0.547, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.802, Trained Tokens 12532852, Peak mem 159.323 GB\n",
      "Iter 4710: Train loss 0.576, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 464.123, Trained Tokens 12559680, Peak mem 159.323 GB\n",
      "Iter 4720: Train loss 0.554, Learning Rate 1.000e-05, It/sec 0.195, Tokens/sec 515.222, Trained Tokens 12586062, Peak mem 159.323 GB\n",
      "Iter 4730: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.782, Trained Tokens 12612698, Peak mem 159.323 GB\n",
      "Iter 4740: Train loss 0.562, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.241, Trained Tokens 12639291, Peak mem 159.323 GB\n",
      "Iter 4750: Train loss 0.586, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.893, Trained Tokens 12665844, Peak mem 159.323 GB\n",
      "Iter 4760: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 479.594, Trained Tokens 12692942, Peak mem 159.323 GB\n",
      "Iter 4770: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.258, Trained Tokens 12719703, Peak mem 159.323 GB\n",
      "Iter 4780: Train loss 0.576, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 469.024, Trained Tokens 12746183, Peak mem 159.323 GB\n",
      "Iter 4790: Train loss 0.602, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 496.444, Trained Tokens 12772906, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 4800: Val loss 0.685, Val took 94.866s\n",
      "Iter 4800: Train loss 0.536, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.771, Trained Tokens 12799494, Peak mem 159.323 GB\n",
      "Iter 4810: Train loss 0.579, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 492.496, Trained Tokens 12826014, Peak mem 159.323 GB\n",
      "Iter 4820: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.814, Trained Tokens 12852635, Peak mem 159.323 GB\n",
      "Iter 4830: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.703, Trained Tokens 12879224, Peak mem 159.323 GB\n",
      "Iter 4840: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 479.654, Trained Tokens 12905659, Peak mem 159.323 GB\n",
      "Iter 4850: Train loss 0.563, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 477.235, Trained Tokens 12932630, Peak mem 159.323 GB\n",
      "Iter 4860: Train loss 0.561, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.912, Trained Tokens 12959445, Peak mem 159.323 GB\n",
      "Iter 4870: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 472.909, Trained Tokens 12986209, Peak mem 159.323 GB\n",
      "Iter 4880: Train loss 0.573, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 454.657, Trained Tokens 13013112, Peak mem 159.323 GB\n",
      "Iter 4890: Train loss 0.551, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.107, Trained Tokens 13039775, Peak mem 159.323 GB\n",
      "Iter 4900: Train loss 0.606, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 471.279, Trained Tokens 13066414, Peak mem 159.323 GB\n",
      "Iter 4910: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 504.953, Trained Tokens 13092922, Peak mem 159.323 GB\n",
      "Iter 4912: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0004912_adapters.safetensors.\n",
      "Iter 4920: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.411, Trained Tokens 13119516, Peak mem 159.323 GB\n",
      "Iter 4930: Train loss 0.579, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.544, Trained Tokens 13146054, Peak mem 159.323 GB\n",
      "Iter 4940: Train loss 0.582, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.040, Trained Tokens 13172755, Peak mem 159.323 GB\n",
      "Iter 4950: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 488.276, Trained Tokens 13199690, Peak mem 159.323 GB\n",
      "Iter 4960: Train loss 0.556, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.644, Trained Tokens 13226328, Peak mem 159.323 GB\n",
      "Iter 4970: Train loss 0.579, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 482.576, Trained Tokens 13252924, Peak mem 159.323 GB\n",
      "Iter 4980: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 476.914, Trained Tokens 13279199, Peak mem 159.323 GB\n",
      "Iter 4990: Train loss 0.578, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 473.242, Trained Tokens 13305928, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 5000: Val loss 0.679, Val took 94.868s\n",
      "Iter 5000: Train loss 0.577, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.643, Trained Tokens 13332583, Peak mem 159.323 GB\n",
      "Iter 5010: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 507.802, Trained Tokens 13359259, Peak mem 159.323 GB\n",
      "Iter 5020: Train loss 0.586, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.609, Trained Tokens 13385820, Peak mem 159.323 GB\n",
      "Iter 5030: Train loss 0.551, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 476.306, Trained Tokens 13412132, Peak mem 159.323 GB\n",
      "Iter 5040: Train loss 0.577, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 504.501, Trained Tokens 13438621, Peak mem 159.323 GB\n",
      "Iter 5050: Train loss 0.554, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.643, Trained Tokens 13465184, Peak mem 159.323 GB\n",
      "Iter 5060: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.914, Trained Tokens 13491897, Peak mem 159.323 GB\n",
      "Iter 5070: Train loss 0.573, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 505.959, Trained Tokens 13518500, Peak mem 159.323 GB\n",
      "Iter 5080: Train loss 0.496, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.845, Trained Tokens 13545110, Peak mem 159.323 GB\n",
      "Iter 5090: Train loss 0.577, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.653, Trained Tokens 13571659, Peak mem 159.323 GB\n",
      "Iter 5100: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 489.766, Trained Tokens 13598661, Peak mem 159.323 GB\n",
      "Iter 5110: Train loss 0.598, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 464.437, Trained Tokens 13625567, Peak mem 159.323 GB\n",
      "Iter 5120: Train loss 0.560, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 502.423, Trained Tokens 13651953, Peak mem 159.323 GB\n",
      "Iter 5130: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 486.312, Trained Tokens 13678795, Peak mem 159.323 GB\n",
      "Iter 5140: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 495.497, Trained Tokens 13705460, Peak mem 159.323 GB\n",
      "Iter 5150: Train loss 0.554, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 503.551, Trained Tokens 13731894, Peak mem 159.323 GB\n",
      "Iter 5160: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 479.982, Trained Tokens 13758336, Peak mem 159.323 GB\n",
      "Iter 5170: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 453.338, Trained Tokens 13785170, Peak mem 159.323 GB\n",
      "Iter 5180: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 470.905, Trained Tokens 13811786, Peak mem 159.323 GB\n",
      "Iter 5190: Train loss 0.586, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 497.896, Trained Tokens 13838571, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:33<00:00,  3.76s/it]\n",
      "Iter 5200: Val loss 0.660, Val took 93.951s\n",
      "Iter 5200: Train loss 0.539, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 479.835, Trained Tokens 13865022, Peak mem 159.323 GB\n",
      "Iter 5210: Train loss 0.551, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.835, Trained Tokens 13891583, Peak mem 159.323 GB\n",
      "Iter 5220: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 493.364, Trained Tokens 13918117, Peak mem 159.323 GB\n",
      "Iter 5230: Train loss 0.579, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.487, Trained Tokens 13944812, Peak mem 159.323 GB\n",
      "Iter 5240: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 511.775, Trained Tokens 13971690, Peak mem 159.323 GB\n",
      "Iter 5250: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 476.364, Trained Tokens 13998615, Peak mem 159.323 GB\n",
      "Iter 5260: Train loss 0.572, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 492.736, Trained Tokens 14025124, Peak mem 159.323 GB\n",
      "Iter 5270: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 484.284, Trained Tokens 14051843, Peak mem 159.323 GB\n",
      "Iter 5280: Train loss 0.616, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.249, Trained Tokens 14078429, Peak mem 159.323 GB\n",
      "Iter 5290: Train loss 0.556, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 507.715, Trained Tokens 14105083, Peak mem 159.323 GB\n",
      "Iter 5300: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 458.593, Trained Tokens 14131612, Peak mem 159.323 GB\n",
      "Iter 5310: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.634, Trained Tokens 14158171, Peak mem 159.323 GB\n",
      "Iter 5320: Train loss 0.557, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 483.944, Trained Tokens 14184847, Peak mem 159.323 GB\n",
      "Iter 5330: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.724, Trained Tokens 14211430, Peak mem 159.323 GB\n",
      "Iter 5340: Train loss 0.604, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 460.279, Trained Tokens 14238033, Peak mem 159.323 GB\n",
      "Iter 5350: Train loss 0.576, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.480, Trained Tokens 14264791, Peak mem 159.323 GB\n",
      "Iter 5360: Train loss 0.577, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 485.796, Trained Tokens 14291584, Peak mem 159.323 GB\n",
      "Iter 5370: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 494.275, Trained Tokens 14318181, Peak mem 159.323 GB\n",
      "Iter 5380: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 452.540, Trained Tokens 14344977, Peak mem 159.323 GB\n",
      "Iter 5390: Train loss 0.572, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 477.238, Trained Tokens 14371913, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:35<00:00,  3.83s/it]\n",
      "Iter 5400: Val loss 0.679, Val took 95.772s\n",
      "Iter 5400: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 481.088, Trained Tokens 14398425, Peak mem 159.323 GB\n",
      "Iter 5410: Train loss 0.574, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 488.641, Trained Tokens 14424777, Peak mem 159.323 GB\n",
      "Iter 5420: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 485.332, Trained Tokens 14451461, Peak mem 159.323 GB\n",
      "Iter 5430: Train loss 0.544, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 471.407, Trained Tokens 14477908, Peak mem 159.323 GB\n",
      "Iter 5440: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 493.310, Trained Tokens 14504287, Peak mem 159.323 GB\n",
      "Iter 5450: Train loss 0.534, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 475.272, Trained Tokens 14530931, Peak mem 159.323 GB\n",
      "Iter 5460: Train loss 0.590, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 479.431, Trained Tokens 14557813, Peak mem 159.323 GB\n",
      "Iter 5470: Train loss 0.527, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 483.014, Trained Tokens 14584256, Peak mem 159.323 GB\n",
      "Iter 5480: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.163, Tokens/sec 435.183, Trained Tokens 14611031, Peak mem 159.323 GB\n",
      "Iter 5490: Train loss 0.543, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 463.747, Trained Tokens 14637648, Peak mem 159.323 GB\n",
      "Iter 5500: Train loss 0.556, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 497.348, Trained Tokens 14664296, Peak mem 159.323 GB\n",
      "Iter 5510: Train loss 0.551, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 483.307, Trained Tokens 14690783, Peak mem 159.323 GB\n",
      "Iter 5520: Train loss 0.552, Learning Rate 1.000e-05, It/sec 0.196, Tokens/sec 519.964, Trained Tokens 14717260, Peak mem 159.323 GB\n",
      "Iter 5526: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0005526_adapters.safetensors.\n",
      "Iter 5530: Train loss 0.537, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 495.044, Trained Tokens 14743757, Peak mem 159.323 GB\n",
      "Iter 5540: Train loss 0.581, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 484.497, Trained Tokens 14770310, Peak mem 159.323 GB\n",
      "Iter 5550: Train loss 0.549, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 497.473, Trained Tokens 14796884, Peak mem 159.323 GB\n",
      "Iter 5560: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 487.116, Trained Tokens 14823593, Peak mem 159.323 GB\n",
      "Iter 5570: Train loss 0.591, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 486.597, Trained Tokens 14850221, Peak mem 159.323 GB\n",
      "Iter 5580: Train loss 0.548, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 486.252, Trained Tokens 14876915, Peak mem 159.323 GB\n",
      "Iter 5590: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 487.011, Trained Tokens 14903626, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:36<00:00,  3.86s/it]\n",
      "Iter 5600: Val loss 0.671, Val took 96.426s\n",
      "Iter 5600: Train loss 0.556, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 471.031, Trained Tokens 14930061, Peak mem 159.323 GB\n",
      "Iter 5610: Train loss 0.531, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 470.402, Trained Tokens 14956462, Peak mem 159.323 GB\n",
      "Iter 5620: Train loss 0.563, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 474.276, Trained Tokens 14983039, Peak mem 159.323 GB\n",
      "Iter 5630: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 498.791, Trained Tokens 15009672, Peak mem 159.323 GB\n",
      "Iter 5640: Train loss 0.550, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 498.647, Trained Tokens 15036293, Peak mem 159.323 GB\n",
      "Iter 5650: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 480.709, Trained Tokens 15063239, Peak mem 159.323 GB\n",
      "Iter 5660: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 498.701, Trained Tokens 15089871, Peak mem 159.323 GB\n",
      "Iter 5670: Train loss 0.562, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 476.764, Trained Tokens 15116591, Peak mem 159.323 GB\n",
      "Iter 5680: Train loss 0.580, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 473.947, Trained Tokens 15143156, Peak mem 159.323 GB\n",
      "Iter 5690: Train loss 0.578, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 496.513, Trained Tokens 15170308, Peak mem 159.323 GB\n",
      "Iter 5700: Train loss 0.562, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 511.426, Trained Tokens 15196959, Peak mem 159.323 GB\n",
      "Iter 5710: Train loss 0.554, Learning Rate 1.000e-05, It/sec 0.170, Tokens/sec 457.397, Trained Tokens 15223850, Peak mem 159.323 GB\n",
      "Iter 5720: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 486.336, Trained Tokens 15250448, Peak mem 159.323 GB\n",
      "Iter 5730: Train loss 0.542, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 464.742, Trained Tokens 15277131, Peak mem 159.323 GB\n",
      "Iter 5740: Train loss 0.581, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 469.546, Trained Tokens 15304086, Peak mem 159.323 GB\n",
      "Iter 5750: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 484.573, Trained Tokens 15330592, Peak mem 159.323 GB\n",
      "Iter 5760: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 476.550, Trained Tokens 15357312, Peak mem 159.323 GB\n",
      "Iter 5770: Train loss 0.576, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 466.067, Trained Tokens 15384051, Peak mem 159.323 GB\n",
      "Iter 5780: Train loss 0.560, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 487.114, Trained Tokens 15410726, Peak mem 159.323 GB\n",
      "Iter 5790: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 488.395, Trained Tokens 15437435, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:31<00:00,  3.67s/it]\n",
      "Iter 5800: Val loss 0.646, Val took 91.876s\n",
      "Iter 5800: Train loss 0.542, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 502.652, Trained Tokens 15464283, Peak mem 159.323 GB\n",
      "Iter 5810: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 469.378, Trained Tokens 15491241, Peak mem 159.323 GB\n",
      "Iter 5820: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 486.306, Trained Tokens 15517841, Peak mem 159.323 GB\n",
      "Iter 5830: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 469.357, Trained Tokens 15544771, Peak mem 159.323 GB\n",
      "Iter 5840: Train loss 0.577, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 489.565, Trained Tokens 15571586, Peak mem 159.323 GB\n",
      "Iter 5850: Train loss 0.528, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 485.715, Trained Tokens 15598186, Peak mem 159.323 GB\n",
      "Iter 5860: Train loss 0.593, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 510.715, Trained Tokens 15624804, Peak mem 159.323 GB\n",
      "Iter 5870: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 482.740, Trained Tokens 15651863, Peak mem 159.323 GB\n",
      "Iter 5880: Train loss 0.549, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 486.239, Trained Tokens 15678460, Peak mem 159.323 GB\n",
      "Iter 5890: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 503.751, Trained Tokens 15704706, Peak mem 159.323 GB\n",
      "Iter 5900: Train loss 0.546, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 497.507, Trained Tokens 15731308, Peak mem 159.323 GB\n",
      "Iter 5910: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 500.011, Trained Tokens 15758009, Peak mem 159.323 GB\n",
      "Iter 5920: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 475.492, Trained Tokens 15784673, Peak mem 159.323 GB\n",
      "Iter 5930: Train loss 0.573, Learning Rate 1.000e-05, It/sec 0.120, Tokens/sec 328.073, Trained Tokens 15812017, Peak mem 159.323 GB\n",
      "Iter 5940: Train loss 0.568, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 487.788, Trained Tokens 15838691, Peak mem 159.323 GB\n",
      "Iter 5950: Train loss 0.586, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 490.844, Trained Tokens 15864907, Peak mem 159.323 GB\n",
      "Iter 5960: Train loss 0.558, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 466.735, Trained Tokens 15891728, Peak mem 159.323 GB\n",
      "Iter 5970: Train loss 0.618, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 473.561, Trained Tokens 15918298, Peak mem 159.323 GB\n",
      "Iter 5980: Train loss 0.561, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 493.710, Trained Tokens 15944656, Peak mem 159.323 GB\n",
      "Iter 5990: Train loss 0.563, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 494.402, Trained Tokens 15971086, Peak mem 159.323 GB\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.79s/it]\n",
      "Iter 6000: Val loss 0.652, Val took 94.642s\n",
      "Iter 6000: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 499.865, Trained Tokens 15997786, Peak mem 159.323 GB\n",
      "Iter 6010: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 487.342, Trained Tokens 16024474, Peak mem 159.323 GB\n",
      "Iter 6020: Train loss 0.536, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 473.235, Trained Tokens 16051031, Peak mem 159.323 GB\n",
      "Iter 6030: Train loss 0.547, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 492.141, Trained Tokens 16077955, Peak mem 159.323 GB\n",
      "Iter 6040: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 487.465, Trained Tokens 16104614, Peak mem 159.323 GB\n",
      "Iter 6050: Train loss 0.563, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 492.587, Trained Tokens 16130920, Peak mem 159.323 GB\n",
      "Iter 6060: Train loss 0.581, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 488.273, Trained Tokens 16157633, Peak mem 159.323 GB\n",
      "Iter 6070: Train loss 0.556, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 495.687, Trained Tokens 16184102, Peak mem 159.323 GB\n",
      "Iter 6080: Train loss 0.535, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 471.973, Trained Tokens 16210558, Peak mem 159.323 GB\n",
      "Iter 6090: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 464.391, Trained Tokens 16237224, Peak mem 159.323 GB\n",
      "Iter 6100: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.166, Tokens/sec 445.716, Trained Tokens 16264046, Peak mem 159.323 GB\n",
      "Iter 6110: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 491.844, Trained Tokens 16290949, Peak mem 159.323 GB\n",
      "Iter 6120: Train loss 0.585, Learning Rate 1.000e-05, It/sec 0.170, Tokens/sec 455.411, Trained Tokens 16317706, Peak mem 159.323 GB\n",
      "Iter 6130: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 497.764, Trained Tokens 16344288, Peak mem 159.323 GB\n",
      "Iter 6140: Train loss 0.579, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 490.029, Trained Tokens 16371094, Peak mem 159.323 GB\n",
      "Iter 6140: Saved adapter weights to adapters_v3/adapters.safetensors and adapters_v3/0006140_adapters.safetensors.\n",
      "Calculating loss...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:34<00:00,  3.78s/it]\n",
      "Iter 6146: Val loss 0.665, Val took 94.619s\n",
      "Iter 6146: Train loss 0.525, Learning Rate 1.000e-05, It/sec 0.271, Tokens/sec 440.407, Trained Tokens 16387345, Peak mem 159.323 GB\n",
      "Saved final weights to adapters_v3/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "# Use the MLX CLI for LoRA fine-tuning.\n",
    "# Model: mlx-community/Qwen3-30B-A3B-4bit\n",
    "model_name = \"mlx-community/Qwen3-30B-A3B-4bit\"\n",
    "\n",
    "# Training Configuration\n",
    "train_count = 65565  # With balanced (oversampled) data\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "iters = int((train_count / batch_size) * epochs)\n",
    "print(f\"Training for {iters} iterations ({epochs} epoch(s) with batch size {batch_size})\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "\n",
    "# Save checkpoints 10 times during training\n",
    "save_every = max(100, iters // 10)\n",
    "\n",
    "# Save to new adapter path to preserve original training results\n",
    "!python -m mlx_lm.lora \\\n",
    "    --model {model_name} \\\n",
    "    --train \\\n",
    "    --data mlx_data \\\n",
    "    --iters {iters} \\\n",
    "    --batch-size {batch_size} \\\n",
    "    --num-layers 16 \\\n",
    "    --learning-rate {learning_rate} \\\n",
    "    --adapter-path adapters_v3 \\\n",
    "    --save-every {save_every} \\\n",
    "    2>&1 | tee MLX_finetuning_log_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model mlx-community/Qwen3-30B-A3B-4bit with adapters from adapters_v3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0cd22909984cc2a0d3d1441a71364e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Generating predictions for 16281 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16281/16281 [2:04:25<00:00,  2.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission_mlx_v3.csv\n",
      "         id emotion\n",
      "0  0x61fc95    fear\n",
      "1  0xaba820     joy\n",
      "2  0x66e44d     joy\n",
      "3  0xc03cf5     joy\n",
      "4  0x02f65a     joy\n",
      "\n",
      "Emotion distribution in predictions:\n",
      "emotion\n",
      "joy         8156\n",
      "anger       4245\n",
      "surprise    1687\n",
      "sadness     1360\n",
      "fear         751\n",
      "disgust       82\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "model_name = \"mlx-community/Qwen3-30B-A3B-4bit\"\n",
    "adapter_path = \"adapters_v3\"\n",
    "data_dir = 'data/dm-lab-2-private-competition'\n",
    "ids_path = os.path.join(data_dir, 'data_identification.csv')\n",
    "posts_path = os.path.join(data_dir, 'final_posts.json')\n",
    "submission_file = 'submission_mlx_v3.csv'\n",
    "\n",
    "print(f\"Loading model {model_name} with adapters from {adapter_path}...\")\n",
    "try:\n",
    "    model, tokenizer = load(model_name, adapter_path=adapter_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # Fallback or stop\n",
    "    raise e\n",
    "\n",
    "# Load Test Data\n",
    "print(\"Loading test data...\")\n",
    "df_ids = pd.read_csv(ids_path)\n",
    "test_ids = df_ids[df_ids['split'] == 'test']['id'].tolist()\n",
    "\n",
    "with open(posts_path, 'r') as f:\n",
    "    all_posts = json.load(f)\n",
    "\n",
    "# Map ID to Text\n",
    "id_to_text = {}\n",
    "for p in all_posts:\n",
    "    pid = p['root']['_source']['post']['post_id']\n",
    "    if pid in test_ids:\n",
    "        id_to_text[pid] = p['root']['_source']['post']['text']\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = \"You are a professional emotion analysis assistant. Analyze the emotion of the user's text and categorize it as one of the following: anger, disgust, fear, joy, sadness, surprise. Only output the category.\"\n",
    "\n",
    "results = []\n",
    "valid_labels = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "print(f\"Generating predictions for {len(test_ids)} samples...\")\n",
    "\n",
    "for pid in tqdm(test_ids):\n",
    "    text = id_to_text.get(pid, \"\")\n",
    "    \n",
    "    # Construct messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        # Fallback if no chat template\n",
    "        prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    # Generate\n",
    "    response = generate(model, tokenizer, prompt=prompt, max_tokens=20, verbose=False)\n",
    "    \n",
    "    # Post-process\n",
    "    pred_text = response.strip().lower()\n",
    "    \n",
    "    # Extract label\n",
    "    found_label = \"joy\" # Default\n",
    "    for label in valid_labels:\n",
    "        # Check if label is in the response (word boundary check is better but simple contains is often enough for this task)\n",
    "        if label in pred_text:\n",
    "            found_label = label\n",
    "            break\n",
    "            \n",
    "    results.append({'id': pid, 'emotion': found_label})\n",
    "\n",
    "# Save Submission\n",
    "df_sub = pd.DataFrame(results)\n",
    "df_sub.to_csv(submission_file, index=False)\n",
    "print(f\"Submission saved to {submission_file}\")\n",
    "print(df_sub.head())\n",
    "print(f\"\\nEmotion distribution in predictions:\")\n",
    "print(df_sub['emotion'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm_lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
